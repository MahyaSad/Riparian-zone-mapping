Dual-Stream UNET3D for Satellite Image Segmentation

This repository contains a distributed training pipeline for a dual-stream satellite image segmentation model. The implementation leverages PyTorch's DistributedDataParallel (DDP), GPU-accelerated preprocessing using CuPy, and hyperparameter optimization via Optuna.

Note: The provided model (DualStreamUNET3D) is a placeholder. Replace it with your actual model architecture when deploying in production.
Overview

The code in this repository performs the following tasks:

Data Loading & Preprocessing:
Reads satellite data from zarr files (for both Sentinel-1 and Sentinel-2 datasets).
Calculates normalization statistics (mean and standard deviation) using GPU acceleration (via CuPy).
Pads time-series data to a consistent maximum number of time steps across tiles.
Computes class weights from label data for imbalanced segmentation tasks.
Model Definition:
Implements a dummy dual-stream model that concatenates inputs from two satellite data streams and applies a 3D convolution to generate a segmentation output.
Replace this placeholder with your custom model architecture if needed.
Distributed Training:
Sets up a distributed training environment using PyTorch's NCCL backend.
Utilizes DistributedDataParallel (DDP) to efficiently train the model across multiple GPUs.
Employs a training loop with early stopping based on a patience counter and gradient clipping.
Saves the best model checkpoint (best_model.pth) based on validation loss.
Hyperparameter Optimization:
Uses Optuna to explore various hyperparameter configurations (e.g., number of layers, learning rate, batch size).
The objective function trains the model and returns the best validation loss, which Optuna uses to guide the search.
Saves study state in a SQLite database and a pickle file for persistence.
Evaluation:
Loads the best model from the checkpoint.
Evaluates model performance on a test dataset and computes metrics such as loss and accuracy.
Final results (best hyperparameters, test metrics, configuration) are saved to results.json.
Repository Structure

├── README.md           # This file
├── main.py             # Main training pipeline script
├── model.py            # (Optional) Model definition file
├── dataset.py          # (Optional) Dataset and preprocessing code
├── requirements.txt    # List of Python dependencies
└── study.db            # SQLite database for Optuna (created after first run)
Requirements

Python 3.7+
PyTorch with CUDA support
CuPy
zarr
optuna
numpy
tqdm
Install the required packages using:

pip install -r requirements.txt
Example requirements.txt:

torch
torchvision
cupy-cudaXXX  # Replace XXX with your CUDA version (e.g., cupy-cuda11x)
zarr
optuna
numpy
tqdm
Usage

Configure Paths and Tile Identifiers:
Modify the paths and tile identifiers for training, validation, and testing data inside main.py (or your configuration file):

base_train_path = '/path/to/train/data'
base_val_path = '/path/to/val/data'
base_test_path = '/path/to/test/data'
train_tiles = ['tile1', 'tile2', 'tile3']
val_tiles = ['tile4', 'tile5']
test_tiles = ['tile6', 'tile7']
Run Hyperparameter Optimization and Training:
Execute the main script:

python main.py
The script will:

Launch distributed training across available GPUs.
Optimize hyperparameters using Optuna.
Save the best model as best_model.pth and results to results.json.
Evaluation:
After training, the model is evaluated on the test dataset, and performance metrics are printed to the console.
Detailed Workflow

Data Preprocessing
Normalization & Statistics:
The dataset calculates per-variable statistics (mean & std) using CuPy.
Normalizes each variable (e.g., Sentinel-1 and Sentinel-2 bands) with z-score normalization.
Padding:
Pads the time-series data to ensure a consistent time dimension (self.max_T) across all tiles.
Class Weights:
Computes class weights from label data to address class imbalance using compute_class_weights.
Model and Training
Model:
DualStreamUNET3D is a placeholder that concatenates the two streams and processes the combined tensor using a 3D convolution.
Distributed Training:
The training function uses PyTorch's mp.spawn to create processes for each GPU.
Utilizes DistributedDataParallel (DDP) for synchronized multi-GPU training.
Implements gradient clipping and a cosine annealing learning rate scheduler.
Hyperparameter Optimization with Optuna
Objective Function:
Samples various hyperparameters (e.g., number of layers, learning rate, dropout rate) via Optuna.
Runs distributed training for each trial and returns the best validation loss.
Study Persistence:
Saves the study state to a SQLite database (study.db) and a pickle file (study.pkl).
